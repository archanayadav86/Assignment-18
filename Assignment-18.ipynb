{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is the difference between supervised and unsupervised learning? Give some examples to illustrate your point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> Supervised learning uses labeled input and output data, while an unsupervised learning algorithm does not. In supervised learning, the algorithm “learns” from the training dataset by iteratively making predictions on the data and adjusting for the correct answer.\n",
    "One practical example of supervised learning problems is predicting house prices. How is this achieved? First, we need data about the houses: square footage, number of rooms, features, whether a house has a garden or not, and so on. We then need to know the prices of these houses, i.e. the corresponding labels.\n",
    "Some examples of unsupervised learning algorithms include K-Means Clustering, Principal Component Analysis and Hierarchical Clustering we use these algorithms to predict output in unsupervised learning as we don't have labelled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Mention a few unsupervised learning applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> Unsupervised learning models are utilized for three main tasks—clustering, association, and dimensionality reduction. Below we'll define each learning method and highlight common algorithms and approaches to conduct them effectively.\n",
    "\n",
    "Some use cases for unsupervised learning — more specifically, clustering — include: Customer segmentation, or understanding different customer groups around which to build marketing or other business strategies. Genetics, for example clustering DNA patterns to analyze evolutionary biology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. What are the three main types of clustering methods? Briefly describe the characteristics of each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> Types of Clustering -\n",
    "1.Centroid-based Clustering - \n",
    "Centroid-based clustering organizes the data into non-hierarchical clusters, in contrast to hierarchical clustering defined below. k-means is the most widely-used centroid-based clustering algorithm. Centroid-based algorithms are efficient but sensitive to initial conditions and outliers\n",
    "2.Density-based Clustering - \n",
    "In this method, the clusters are created based upon the density of the data points which are represented in the data space. The regions that become dense due to the huge number of data points residing in that region are considered as clusters. The data points in the sparse region are considered as noise or outliers. The clusters created in these methods can be of arbitrary shape. Following are the examples of Density-based clustering algorithms:\n",
    "3.Distribution-based Clustering -\n",
    "As distance from the distribution's center increases, the probability that a point belongs to the distribution decreases. The bands show that decrease in probability. When you do not know the type of distribution in your data, you should use a different algorithm.\n",
    "4.Hierarchical Clustering - \n",
    "Hierarchical Clustering groups (Agglomerative or also called as Bottom-Up Approach) or divides (Divisive or also called as Top-Down Approach) the clusters based on the distance metrics.In agglomerative clustering, initially, each data point acts as a cluster, and then it groups the clusters one by one. This comes under in one of the most sought-after clustering methods.\n",
    "Divisive is the opposite of Agglomerative, it starts off with all the points into one cluster and divides them to create more clusters. These algorithms create a distance matrix of all the existing clusters and perform the linkage between the clusters depending on the criteria of the linkage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Explain how the k-means algorithm determines the consistency of clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans-->It is an iterative process of assigning each data point to the groups and slowly data points get clustered based on similar features. The objective is to minimize the sum of distances between the data points and the cluster centroid, to identify the correct group each data point should belong to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. With a simple illustration, explain the key difference between the k-means and k-medoids algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> K-means attempts to minimize the total squared error, while k-medoids minimizes the sum of dissimilarities between points labeled to be in a cluster and a point designated as the center of that cluster. In contrast to the k -means algorithm, k -medoids chooses datapoints as centers.\n",
    "K-medoids is also a partitioning technique of clustering that clusters the data set of n objects into k clusters with k known a priori . A useful tool for determining k is the silhouette."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. What is a dendrogram, and how does it work? Explain how to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> A dendrogram is a tree-like structure that explains the relationship between all the data points in the system. However, like a regular family tree, a dendrogram need not branch out at regular intervals from top to bottom as the vertical direction (y-axis) in it represents the distance between clusters in some metric.\n",
    "The key to interpreting a dendrogram is to focus on the height at which any two objects are joined together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. What exactly is SSE? What role does it play in the k-means algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> The SSE is defined as the sum of the squared Euclidean distances of each point to its closest centroid. Since this is a measure of error, the objective of k-means is to try to minimize this value. \n",
    "Intra-cluster variance (a.k.a., the squared error function or sum of squares within (SSW) or sum of squares error (SSE)) is used to quantify internal cohesion. It is defined as the sum of the squared distance between the average point (called Centroid) and each point of the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. With a step-by-step algorithm, explain the k-means procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> We have these 8 points, and we want to apply k-means to create clusters for these points. Here’s how we can do it.\n",
    "\n",
    "Step 1: Choose the number of clusters k\n",
    "\n",
    "The first step in k-means is to pick the number of clusters, k.\n",
    "\n",
    "Step 2: Select k random points from the data as centroids\n",
    "\n",
    "Next, we randomly select the centroid for each cluster. Let’s say we want to have 2 clusters, so k is equal to 2 here. We then randomly select the centroid:\n",
    "\n",
    "Here, the red and green circles represent the centroid for these clusters.\n",
    "\n",
    "Step 3: Assign all the points to the closest cluster centroid\n",
    "\n",
    "Once we have initialized the centroids, we assign each point to the closest cluster centroid:\n",
    "    \n",
    "Step 4: Recompute the centroids of newly formed clusters\n",
    "\n",
    "Now, once we have assigned all of the points to either cluster, the next step is to compute the centroids of newly formed clusters:\n",
    "\n",
    "Step 5: Repeat steps 3 and 4\n",
    "\n",
    "We then repeat steps 3 and 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9. In the sense of hierarchical clustering, define the terms single link and complete link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> Single-link clusters at step are maximal sets of points that are linked via at least one link (a single link) of similarity \n",
    "Complete-link clusters at step are maximal sets of points that are completely linked with each other via links of similarity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10. How does the apriori concept aid in the reduction of measurement overhead in a business basket analysis? Give an example to demonstrate your point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> The Apriori algorithm is commonly cited by data scientists in research articles about market basket analysis. It identifies frequent items in the database and then evaluates their frequency as the datasets are expanded to larger sizes.\n",
    "\n",
    "Apriori algorithm assumes that any subset of a frequent itemset must be frequent. Its the algorithm behind Market Basket Analysis.\n",
    "\n",
    "We can optimize the existing apriori algorithm by which it will take less time and also works with less memory using these methods:\n",
    "\n",
    "Hash-based itemset counting: A k-itemset whose corresponding hashing bucket count is below the threshold cannot be frequent.\n",
    "Transaction reduction: A transaction that does not contain any frequent k-itemsets useless in subsequent scans.\n",
    "Partitioning: An itemset that is potentially frequent in DB must be frequent in at least one of the partitions of DB.\n",
    "Sampling: Mining on a subset of given data, lower support threshold + a method to determine the completeness.\n",
    "Dynamic itemset counting: add new candidate itemsets only when all of their subsets are estimated to be frequent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
